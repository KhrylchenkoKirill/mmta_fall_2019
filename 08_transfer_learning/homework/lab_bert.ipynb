{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 3 \n",
    "\n",
    "# Классификация предложений с использованием BERT\n",
    "\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "\n",
    "\n",
    "### ФИО: <впишите>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы будете классифицировать предложения из медицинских статей на несколько классов (background, objective и т.д.). \n",
    "Для того, чтобы улучшить качество решения вам предлагается дообучить предобученную нейросетевую архитектуру BERT.\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    " - [Pytorch](https://pytorch.org/).\n",
    " - [Transformers](https://github.com/huggingface/transformers).\n",
    " \n",
    "### Данные\n",
    "\n",
    "Скачать данные можно здесь: [ссылка на google диск](https://drive.google.com/file/d/13HlWH8jnmsxqDKrEptxOXQg9kkuQMmGq/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с предложениями из медицинских статей, разбитых на несколько классов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путь к папке с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"sentence_classification_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция считывания данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Pubmed sentences file path\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    text_data : list of str\n",
    "        List of sentences for algorithm\n",
    "    \n",
    "    target_data : list of str\n",
    "        List of sentence categories\n",
    "    \"\"\"\n",
    "    text_data = []\n",
    "    target_data = []\n",
    "\n",
    "    with open(file_name, 'r') as f_input:\n",
    "        for line in f_input:\n",
    "            if line.startswith('#') or line == '\\n':\n",
    "                continue\n",
    "            target, text = line.split('\\t')[:2]    \n",
    "\n",
    "            text_data.append(text)\n",
    "            target_data.append(target)\n",
    "    \n",
    "    return text_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считывание данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_target = read_data(f'{DATA_PATH}/data_train.txt')\n",
    "test_data, test_target = read_data(f'{DATA_PATH}/data_test.txt')\n",
    "dev_data, dev_target = read_data(f'{DATA_PATH}/data_dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Построение бейзлайна (1 балл)\n",
    "\n",
    "В этой части задания вам необходимо построить бейзлайн модель, с которой вы будете сравнивать ваше решение. В качестве бейзлайна вам предлагается использовать модель логистической регрессии на tf-idf представлениях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как подать в модель предложения, необходимо их предобработать:\n",
    "    \n",
    "1. привести все предложения к нижнему регистру\n",
    "2. удалить из предложений все непробельные символы кроме букв, цифр\n",
    "3. все цифры заменить на нули\n",
    "\n",
    "Метки ответов необходимо преобразовать из текстового вида в числовой (это можно сделать с помощью LabelEncoder).\n",
    "\n",
    "Затем необходимо построить tf-idf матрицу по выбранным предложениям (используйте для подсчёта tf-idf только train_data!) и обучить на них модель логистической регрессии. Используйте dev выборку для подбора гиперпараметров модели. Добейтесь того, что на test и dev выборках accuracy будет будет выше 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Задание BERT (4 балла за 3 и 4 части)\n",
    "\n",
    "Так как обучающих предложений очень мало, попробуем использовать модель BERT, предобученную на большом датасете. Будем использовать библиотеку transformers. Для обучения модели используйте данные до обработки из предыдущего пункта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель BERT работает с специальным форматом данных — все токены из предложения получены с помощью алгоритма BPE. Класс BertTokenizer позволяет получить BPE разбиение для предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке transformers есть специальный класс для работы с задачей классификации — BertForSequenceClassification. Воспользуемся им, чтобы задать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL_NAME, num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "bert_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем специальный кастомный датасет для токенизированных с помощью BPE предложений. Каждое предложение должно быть преобразовано в последовательность BPE индексов. Не забудьте, что в начале каждого предложения должен стоять специальный токен [CLS], а в конце должен стоять специальный токен [SEP].\n",
    "\n",
    "Задайте датасет, используя BertTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenizer, text_data, target_data=None, max_length=256):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokenizer : instance of BertTokenizer\n",
    "        text_data : list of str\n",
    "            List of input sentences\n",
    "        target_data : list of int\n",
    "            List of input targets\n",
    "        max_length : int\n",
    "            Maximum length of input sequence (length in bpe tokens)\n",
    "        \"\"\"\n",
    "        #### your code here ####\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if self.target_data is not None:\n",
    "            return self.data[i], self.target_data[i]\n",
    "        else:\n",
    "            return self.data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получите все датасеты для всех типов данных. \n",
    "\n",
    "**Замечание**. После получения есть смысл сохранить все датасеты на диск, т.к. предобработка занимает время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BertTokenizedDataset(tokenizer, train_data, y_train)\n",
    "dev_dataset = BertTokenizedDataset(tokenizer, dev_data, y_dev)\n",
    "test_dataset = BertTokenizedDataset(tokenizer, test_data, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем  класс PadSequences, чтобы задать способ паддинга, работающий с встроенным в pytorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, use_labels=False):\n",
    "        self.use_labels = use_labels\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : list of objects or list of (object, label)\n",
    "            Each object is list of int indexes.\n",
    "            Each label is int.\n",
    "        \"\"\"\n",
    "        data_label_batch = batch if self.use_labels else [(x, 0) for x in batch]\n",
    "            \n",
    "        # Sort the batch in the descending order\n",
    "        sorted_batch = sorted(data_label_batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "        # Get each sequence and pad it\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "        max_lenght = len(sequences[0])\n",
    "\n",
    "        # Also need to store the length of each sequence\n",
    "        # This is later needed in order to unpad the sequences\n",
    "        lengths = torch.LongTensor([[1] * len(x) + [0] * (max_lenght - len(x)) for x in sequences])\n",
    "        # Don't forget to grab the labels of the *sorted* batch\n",
    "        \n",
    "        if self.use_labels:\n",
    "            labels = torch.LongTensor([x[1] for x in sorted_batch])\n",
    "            return sequences_padded, lengths, labels\n",
    "        else:\n",
    "            return sequences_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим DataLoader для каждого из датасетов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              collate_fn=PadSequences(use_labels=True))\n",
    "\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              collate_fn=PadSequences(use_labels=True))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              collate_fn=PadSequences(use_labels=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что модель трансформера обучается по достаточному большому размеру батча (обычно 64), который скорее всего не будет влезать на вашу видеокарту. Поэтому, рекомендуется \"накапливать\" градиенты за несколько итераций. С помощью параметра ACCUMULATION_STEPS задайте, раз в сколько итераций вам необходимо делать шаг метода оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_AMOUNT = 2\n",
    "TRAIN_LENGTH = len(train_dataset)\n",
    "BATCH_SIZE = 16\n",
    "ACCUMULATION_STEPS = 4\n",
    "\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте общее число раз, когда ваш оптимизатор будет делать обновления на основе выбранных значений EPOCH_AMOUNT, BATCH_SIZE, ACCUMULATION_STEPS и  TRAIN_LENGTH. Эта величина нужна для правильного задания параметров оптимизаторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_optimization_step_amount = ### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим параметры оптимизаторов. Мы будем использовать специальные оптимизаторы из библиотеки transformers AdamW и WarmupLinearSchedule, обеспечивающие плавный разгон и медленное затухание темпа обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert_model.parameters(), lr=LR, correct_bias=False)\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer,\n",
    "    warmup_steps=train_optimization_step_amount * 0.05,\n",
    "    t_total=train_optimization_step_amount,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для некоторых групп параметров зададим коэффициенты регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(bert_model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Обучение BERT \n",
    "\n",
    "Теперь всё готово к тому, чтобы дообучить BERT на датасете train_dataset!\n",
    "\n",
    "Используйте dev_dataset для выбора гиперпараметров модели и обучения. Задание будет засчтано на полный балл если на dev_dataset и test_dataset точность будет выше 0.84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть (до 3 баллов)\n",
    "\n",
    "Улучшите качество (на обеих выборках), используя любые способы (кроме использования дополнительных обучающих данных датасета RCT2000):\n",
    "\n",
    "* $> 0.86$ — 1 балл \n",
    "* $> 0.88$ — 2 балла\n",
    "* $> 0.9$ — 3 балла"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_py3",
   "language": "python",
   "name": "base_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
